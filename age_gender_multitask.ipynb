{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGE & GENDER DETECTION MULTITASK MODEL\n",
    "This model helps us to make our single custom neural network that can simultaneously do two seperate tasks of detecting gender and age at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import os\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D, ZeroPadding2D\n",
    "from keras import applications\n",
    "#from keras.utils import plot_model, to_categorical\n",
    "from keras.applications import VGG16, ResNet50\n",
    "from keras.optimizers import RMSprop,Adam  \n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "import pickle\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 23708\n",
      "28_0_3_20170119194507123.jpg.chip.jpg\n"
     ]
    }
   ],
   "source": [
    "path = \"train/\"\n",
    "files = os.listdir(path)\n",
    "size = len(files)\n",
    "shuffle(files)\n",
    "print(\"Total samples:\",size)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = os.listdir(\"train\")\n",
    "#shuffle(files)\n",
    "ages=[]  #Children (1-14) CLASS 0;Youth (14-25) CLASS 1;ADULTS (25-40) CLASS 2;Middle age (40-60) CLASS 3\n",
    "         #Very Old (>60) CLASS 4\n",
    "genders=[] #0=male ;1=female\n",
    "images=[]\n",
    "for file in files:\n",
    "    image= cv2.imread(path+file,1) #1 means colourful\n",
    "    image = cv2.resize(image,dsize=(32,32))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = img_to_array(image)\n",
    "    #image = image.reshape((image.shape[0],image.shape[1],3))\n",
    "    images.append(image)\n",
    "    split_var = file.split('_')\n",
    "    genders.append(int(split_var[1]) )\n",
    "    label_1=file.split('_')[0]\n",
    "    label_1 = int(label_1)\n",
    "    if label_1 <= 14:\n",
    "         ages.append(('0'))\n",
    "    if (label_1>14) and (label_1<=25):\n",
    "        ages.append(('1'))\n",
    "    if (label_1>25) and (label_1<40):\n",
    "        ages.append(('2'))\n",
    "    if (label_1>=40) and (label_1<60):\n",
    "        ages.append(('3'))\n",
    "    if label_1>=60:\n",
    "        ages.append(('4'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(images[7])\n",
    "print(ages[7])\n",
    "print(genders[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images, dtype=\"float\") / 255.0\n",
    " \n",
    "ages = np.array(ages)\n",
    "genders = np.array(genders)\n",
    " \n",
    "agesLB = LabelBinarizer()\n",
    "gendersLB = LabelBinarizer()\n",
    "\n",
    "ages = agesLB.fit_transform(ages)\n",
    "genders = gendersLB.fit_transform(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(images, ages, genders, test_size=0.2, random_state=42)\n",
    "(trainX, testX, trainAgeY, testAgeY, trainGenderY, testGenderY) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' '3' '4']\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(agesLB.classes_)\n",
    "print(gendersLB.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 32\n",
    "img_col = 32\n",
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_row,img_col,3)) #taking pretrained weights;top layer chopped off as i am going to use it of my own;\n",
    "    \n",
    "for layer in pre_trained_model.layers[:15]:\n",
    "    layer.trainable = False  #freeze upto 15th layer only of vgg16\n",
    "\n",
    "for layer in pre_trained_model.layers[15:]:\n",
    "    layer.trainable = False  #if true fine tuning vgg16 starts from this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###OWN MODEL FROM SCRATCH\n",
    "\n",
    "# import keras \n",
    "# from keras.layers import *\n",
    "# from keras.models import *\n",
    "# from keras import backend as K\n",
    "\n",
    "# visible=Input(shape=(32,32,3))\n",
    "# x=Convolution2D(64, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(visible)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "\n",
    "# # Second CONV-ReLU Layer\n",
    "# x=Convolution2D(64, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(x)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "\n",
    "# # Max Pooling with Dropout \n",
    "# x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# x=Dropout(0.2)(x)\n",
    "\n",
    "# # 3rd set of CONV-ReLU Layers\n",
    "# x=Convolution2D(128, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(x)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "# # 4th Set of CONV-ReLU Layers\n",
    "# x=Convolution2D(128, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(x)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "# # Max Pooling with Dropout \n",
    "# x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# x=Dropout(0.2)(x)\n",
    "# # 5th Set of CONV-ReLU Layers\n",
    "# x=Convolution2D(256, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(x)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "\n",
    "# # 6th Set of CONV-ReLU Layers\n",
    "# x=Convolution2D(256, (3, 3), padding = 'same', input_shape = (img_row, img_col, 3))(x)\n",
    "# x=Activation('relu')(x)\n",
    "# x=BatchNormalization()(x)\n",
    "\n",
    "# # Max Pooling with Dropout \n",
    "# x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# top_model=Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = pre_trained_model.output #output of vgg16 last layer taken as input here\n",
    "top_model = Flatten()(top_model)\n",
    "layer_1 = Dense(1024,activation='relu')(top_model)\n",
    "layer_2= Dense(1024,activation='relu')(top_model)\n",
    "layer_1 = Dense(128,activation='relu')(layer_1)\n",
    "layer_1 = Dropout(0.5)(layer_1)\n",
    "layer_2 = Dropout(0.5)(layer_2)\n",
    "out_age = Dense(5,activation='softmax',name='out_age')(layer_1) #for age\n",
    "out_gender = Dense(1,activation='sigmoid',name='out_gender')(layer_2) #for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = pre_trained_model.input ,outputs=[out_age,out_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 32, 32, 64)   1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 32, 32, 64)   36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 16, 16, 64)   0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 16, 16, 128)  73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 16, 16, 128)  147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 8, 8, 128)    0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 8, 8, 256)    295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 8, 8, 256)    590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 8, 8, 256)    590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 4, 4, 256)    0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 4, 4, 512)    1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 4, 4, 512)    2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 4, 4, 512)    2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 2, 2, 512)    0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 2, 2, 512)    2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 2, 2, 512)    2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 2, 2, 512)    2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 1, 1, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         525312      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          131200      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         525312      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_age (Dense)                 (None, 5)            645         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_gender (Dense)              (None, 1)            1025        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,898,182\n",
      "Trainable params: 1,183,494\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',                              #{'out_1':'adam','out_2':'adam'}, \n",
    "              loss={'out_age':'categorical_crossentropy','out_gender': 'binary_crossentropy'},\n",
    "              metrics ={'out_age': 'mae', 'out_gender': 'accuracy'}                                #{'out_1':\"accuracy\",'out_2':\"accuracy\"}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"multitask_age_gender_final.h5\",\n",
    "                             monitor=\"val_accuracy\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18966 samples, validate on 4742 samples\n",
      "Epoch 1/50\n",
      "18966/18966 [==============================] - 44s 2ms/step - loss: 1.8455 - out_age_loss: 1.2928 - out_gender_loss: 0.5525 - out_age_mae: 0.2653 - out_gender_accuracy: 0.7220 - val_loss: 1.6968 - val_out_age_loss: 1.1765 - val_out_gender_loss: 0.5221 - val_out_age_mae: 0.2472 - val_out_gender_accuracy: 0.7421\n",
      "Epoch 2/50\n",
      "  160/18966 [..............................] - ETA: 19s - loss: 1.6998 - out_age_loss: 1.2268 - out_gender_loss: 0.4730 - out_age_mae: 0.2554 - out_gender_accuracy: 0.7437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TOOLS\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.7006 - out_age_loss: 1.1878 - out_gender_loss: 0.5129 - out_age_mae: 0.2489 - out_gender_accuracy: 0.7511 - val_loss: 1.6997 - val_out_age_loss: 1.1826 - val_out_gender_loss: 0.5177 - val_out_age_mae: 0.2466 - val_out_gender_accuracy: 0.7442\n",
      "Epoch 3/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.6490 - out_age_loss: 1.1549 - out_gender_loss: 0.4941 - out_age_mae: 0.2435 - out_gender_accuracy: 0.7585 - val_loss: 1.6227 - val_out_age_loss: 1.1425 - val_out_gender_loss: 0.4829 - val_out_age_mae: 0.2384 - val_out_gender_accuracy: 0.7655\n",
      "Epoch 4/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.6147 - out_age_loss: 1.1288 - out_gender_loss: 0.4861 - out_age_mae: 0.2393 - out_gender_accuracy: 0.7664 - val_loss: 1.6077 - val_out_age_loss: 1.1330 - val_out_gender_loss: 0.4761 - val_out_age_mae: 0.2405 - val_out_gender_accuracy: 0.7659\n",
      "Epoch 5/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.5871 - out_age_loss: 1.1115 - out_gender_loss: 0.4755 - out_age_mae: 0.2366 - out_gender_accuracy: 0.7730 - val_loss: 1.6104 - val_out_age_loss: 1.1411 - val_out_gender_loss: 0.4713 - val_out_age_mae: 0.2396 - val_out_gender_accuracy: 0.7708\n",
      "Epoch 6/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.5515 - out_age_loss: 1.0836 - out_gender_loss: 0.4679 - out_age_mae: 0.2321 - out_gender_accuracy: 0.7769 - val_loss: 1.6242 - val_out_age_loss: 1.1379 - val_out_gender_loss: 0.4881 - val_out_age_mae: 0.2359 - val_out_gender_accuracy: 0.7566\n",
      "Epoch 7/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.5262 - out_age_loss: 1.0678 - out_gender_loss: 0.4585 - out_age_mae: 0.2292 - out_gender_accuracy: 0.7828 - val_loss: 1.6094 - val_out_age_loss: 1.1371 - val_out_gender_loss: 0.4751 - val_out_age_mae: 0.2337 - val_out_gender_accuracy: 0.7710\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.4350 - out_age_loss: 0.9998 - out_gender_loss: 0.4352 - out_age_mae: 0.2197 - out_gender_accuracy: 0.7982 - val_loss: 1.5786 - val_out_age_loss: 1.1226 - val_out_gender_loss: 0.4586 - val_out_age_mae: 0.2261 - val_out_gender_accuracy: 0.7779\n",
      "Epoch 9/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.4016 - out_age_loss: 0.9736 - out_gender_loss: 0.4279 - out_age_mae: 0.2156 - out_gender_accuracy: 0.8028 - val_loss: 1.5809 - val_out_age_loss: 1.1229 - val_out_gender_loss: 0.4597 - val_out_age_mae: 0.2256 - val_out_gender_accuracy: 0.7779\n",
      "Epoch 10/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.3769 - out_age_loss: 0.9593 - out_gender_loss: 0.4176 - out_age_mae: 0.2128 - out_gender_accuracy: 0.8082 - val_loss: 1.5920 - val_out_age_loss: 1.1373 - val_out_gender_loss: 0.4568 - val_out_age_mae: 0.2249 - val_out_gender_accuracy: 0.7805\n",
      "Epoch 11/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.3599 - out_age_loss: 0.9449 - out_gender_loss: 0.4152 - out_age_mae: 0.2103 - out_gender_accuracy: 0.8082 - val_loss: 1.5939 - val_out_age_loss: 1.1382 - val_out_gender_loss: 0.4573 - val_out_age_mae: 0.2234 - val_out_gender_accuracy: 0.7815\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/50\n",
      "18966/18966 [==============================] - 23s 1ms/step - loss: 1.3259 - out_age_loss: 0.9180 - out_gender_loss: 0.4079 - out_age_mae: 0.2067 - out_gender_accuracy: 0.8135 - val_loss: 1.5862 - val_out_age_loss: 1.1324 - val_out_gender_loss: 0.4556 - val_out_age_mae: 0.2236 - val_out_gender_accuracy: 0.7834\n",
      "Epoch 13/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.3149 - out_age_loss: 0.9109 - out_gender_loss: 0.4041 - out_age_mae: 0.2054 - out_gender_accuracy: 0.8141 - val_loss: 1.5884 - val_out_age_loss: 1.1347 - val_out_gender_loss: 0.4553 - val_out_age_mae: 0.2230 - val_out_gender_accuracy: 0.7817\n",
      "Epoch 14/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.3102 - out_age_loss: 0.9079 - out_gender_loss: 0.4023 - out_age_mae: 0.2046 - out_gender_accuracy: 0.8156 - val_loss: 1.5905 - val_out_age_loss: 1.1370 - val_out_gender_loss: 0.4550 - val_out_age_mae: 0.2228 - val_out_gender_accuracy: 0.7819\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 15/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.3052 - out_age_loss: 0.9041 - out_gender_loss: 0.4011 - out_age_mae: 0.2041 - out_gender_accuracy: 0.8200 - val_loss: 1.5905 - val_out_age_loss: 1.1368 - val_out_gender_loss: 0.4552 - val_out_age_mae: 0.2226 - val_out_gender_accuracy: 0.7819\n",
      "Epoch 16/50\n",
      "18966/18966 [==============================] - 23s 1ms/step - loss: 1.3025 - out_age_loss: 0.9017 - out_gender_loss: 0.4009 - out_age_mae: 0.2040 - out_gender_accuracy: 0.8183 - val_loss: 1.5911 - val_out_age_loss: 1.1376 - val_out_gender_loss: 0.4551 - val_out_age_mae: 0.2224 - val_out_gender_accuracy: 0.7828\n",
      "Epoch 17/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2995 - out_age_loss: 0.8986 - out_gender_loss: 0.4010 - out_age_mae: 0.2035 - out_gender_accuracy: 0.8186 - val_loss: 1.5919 - val_out_age_loss: 1.1386 - val_out_gender_loss: 0.4549 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7822\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 18/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2965 - out_age_loss: 0.8977 - out_gender_loss: 0.3987 - out_age_mae: 0.2035 - out_gender_accuracy: 0.8202 - val_loss: 1.5918 - val_out_age_loss: 1.1385 - val_out_gender_loss: 0.4549 - val_out_age_mae: 0.2223 - val_out_gender_accuracy: 0.7822\n",
      "Epoch 19/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2949 - out_age_loss: 0.8963 - out_gender_loss: 0.3984 - out_age_mae: 0.2031 - out_gender_accuracy: 0.8178 - val_loss: 1.5919 - val_out_age_loss: 1.1386 - val_out_gender_loss: 0.4549 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7824\n",
      "Epoch 20/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2989 - out_age_loss: 0.8987 - out_gender_loss: 0.4001 - out_age_mae: 0.2035 - out_gender_accuracy: 0.8188 - val_loss: 1.5920 - val_out_age_loss: 1.1386 - val_out_gender_loss: 0.4550 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7824\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 21/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2957 - out_age_loss: 0.8965 - out_gender_loss: 0.3993 - out_age_mae: 0.2032 - out_gender_accuracy: 0.8193 - val_loss: 1.5920 - val_out_age_loss: 1.1387 - val_out_gender_loss: 0.4550 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7824\n",
      "Epoch 22/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2969 - out_age_loss: 0.8980 - out_gender_loss: 0.3988 - out_age_mae: 0.2031 - out_gender_accuracy: 0.8196 - val_loss: 1.5920 - val_out_age_loss: 1.1387 - val_out_gender_loss: 0.4550 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7824\n",
      "Epoch 23/50\n",
      "18966/18966 [==============================] - 22s 1ms/step - loss: 1.2975 - out_age_loss: 0.8971 - out_gender_loss: 0.4006 - out_age_mae: 0.2033 - out_gender_accuracy: 0.8174 - val_loss: 1.5921 - val_out_age_loss: 1.1387 - val_out_gender_loss: 0.4550 - val_out_age_mae: 0.2222 - val_out_gender_accuracy: 0.7824\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(trainX,\n",
    "\t{\"out_age\": trainAgeY, \"out_gender\": trainGenderY},\n",
    "\tvalidation_data=(testX,\n",
    "\t\t{\"out_age\": testAgeY, \"out_gender\": testGenderY}),\n",
    "\tepochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "\tverbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"age_gender_multitask_1_23epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
