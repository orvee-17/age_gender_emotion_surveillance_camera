{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import os\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D, ZeroPadding2D\n",
    "from keras import applications\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.applications import VGG16, ResNet50\n",
    "from keras.optimizers import RMSprop,Adam \n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"train\")\n",
    "shuffle(filenames)\n",
    "age=[]  #Children (1-14) CLASS 0;Youth (14-25) CLASS 1;ADULTS (25-40) CLASS 2;Middle age (40-60) CLASS 3\n",
    "         #Very Old (>60) CLASS 4\n",
    "gender=[] #0=male ;1=female\n",
    "\n",
    "for filename in filenames:\n",
    "    label_1=filename.split('_')[0]\n",
    "    label_1 = int(label_1)\n",
    "    if label_1 <= 14:\n",
    "        age.append(str(0))\n",
    "    if (label_1>14) and (label_1<=25):\n",
    "        age.append(str(1))\n",
    "    if (label_1>25) and (label_1<40):\n",
    "        age.append(str(2))\n",
    "    if (label_1>=40) and (label_1<60):\n",
    "        age.append(str(3))\n",
    "    if label_1>=60:\n",
    "        age.append(str(4))\n",
    "   \n",
    "for filename in filenames:\n",
    "    label_2=filename.split('_')[1]\n",
    "    label_2 = int(label_2)\n",
    "    if label_2 == 0:\n",
    "        gender.append(str(0))\n",
    "    if label_2 == 1:\n",
    "        gender.append(str(1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23_0_3_20170119164007795.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5_0_3_20161220220614792.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57_1_0_20170109132244248.jpg.chip.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24_1_0_20170104022800709.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48_0_0_20170104182220669.jpg.chip.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename age gender\n",
       "0  23_0_3_20170119164007795.jpg.chip.jpg   1      0\n",
       "1   5_0_3_20161220220614792.jpg.chip.jpg   0      0\n",
       "2  57_1_0_20170109132244248.jpg.chip.jpg   3      1\n",
       "3  24_1_0_20170104022800709.jpg.chip.jpg   1      1\n",
       "4  48_0_0_20170104182220669.jpg.chip.jpg   3      0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({\n",
    "    'filename':filenames,\n",
    "    'age':age,\n",
    "    'gender':gender\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 32\n",
    "img_col = 32\n",
    "epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_row,img_col,3)) #taking pretrained weights;top layer chopped off as i am going to use it of my own;\n",
    "    \n",
    "for layer in pre_trained_model.layers[:15]:\n",
    "    layer.trainable = False  #freeze upto 15th layer only of vgg16\n",
    "\n",
    "for layer in pre_trained_model.layers[15:]:\n",
    "    layer.trainable = False  #if true fine tuning vgg16 starts from this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = pre_trained_model.output #output of vgg16 last layer taken as input here\n",
    "top_model = Flatten()(top_model)\n",
    "layer_1 = Dense(1024,activation='relu')(top_model)\n",
    "layer_2= Dense(1024,activation='relu')(top_model)\n",
    "#layer_1 = Dense(128,activation='relu')(layer_1)\n",
    "layer_1 = Dropout(0.5)(layer_1)\n",
    "layer_2 = Dropout(0.5)(layer_2)\n",
    "out_1 = Dense(5,activation='softmax',name='out_1')(layer_1) #for age\n",
    "out_2 = Dense(1,activation='sigmoid',name='out_2')(layer_2) #for gender\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = pre_trained_model.input ,outputs=out_1) #use out_1 for age and out_2 for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "out_1 (Dense)                (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 15,245,125\n",
      "Trainable params: 530,437\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21337\n",
      "2371\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = train_test_split(df, test_size=0.1)\n",
    "train_df = train_df.reset_index()                #arguement ta has to be a pd.dataframe,np.array or list\n",
    "validate_df = validate_df.reset_index() \n",
    "# validate_df = validate_df.sample(n=100).reset_index() # use for fast testing code purpose\n",
    "# train_df = train_df.sample(n=1800).reset_index() # use for fast testing code purpose\n",
    "\n",
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "\n",
    "print(total_train)\n",
    "print(total_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21337 validated image filenames belonging to 5 classes.\n",
      "Found 2371 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    \"train\", \n",
    "    x_col='filename',\n",
    "    y_col='age',\n",
    "    class_mode='categorical',                    #'multi_output'],\n",
    "    target_size=(img_row, img_col),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    \"train\", #duitai training file theke kintu\n",
    "    x_col='filename',\n",
    "    y_col='age',\n",
    "    class_mode='categorical',\n",
    "    target_size=(img_row, img_col),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21337 validated image filenames belonging to 5 classes.\n",
      "Found 2371 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    \"train\", \n",
    "    x_col='filename',\n",
    "    y_col='age',\n",
    "    class_mode='categorical',                    #'multi_output'],\n",
    "    target_size=(img_row, img_col),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    \"train\", #duitai training file theke kintu\n",
    "    x_col='filename',\n",
    "    y_col='age',\n",
    "    class_mode='categorical',\n",
    "    target_size=(img_row, img_col),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=.1.600000018697756e-7),                              #{'out_1':'adam','out_2':'adam'}, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics =['accuracy']                           #{'out_1':\"accuracy\",'out_2':\"accuracy\"}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,Adam    #import activation function that you need\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau \n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', #helps to monitor if val_loss;if it doesn't reduce after 5 epochs, the learning rate will be decreased and checked again in the same way;this will be done upto lr=.001\n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.0001)\n",
    "\n",
    "callbacks=[learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/166 [==============================] - 60s 359ms/step - loss: 1.3480 - accuracy: 0.4319 - val_loss: 1.2646 - val_accuracy: 0.4549\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - 57s 346ms/step - loss: 1.2759 - accuracy: 0.4548 - val_loss: 1.1228 - val_accuracy: 0.4974\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - 37s 226ms/step - loss: 1.2561 - accuracy: 0.4694 - val_loss: 1.1189 - val_accuracy: 0.5060\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - 38s 226ms/step - loss: 1.2504 - accuracy: 0.4630 - val_loss: 1.2447 - val_accuracy: 0.4887\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.2349 - accuracy: 0.4659 - val_loss: 1.2294 - val_accuracy: 0.5188\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.2396 - accuracy: 0.4674 - val_loss: 1.1690 - val_accuracy: 0.4991\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - 38s 232ms/step - loss: 1.2177 - accuracy: 0.4764 - val_loss: 1.2406 - val_accuracy: 0.5215\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - 38s 229ms/step - loss: 1.2042 - accuracy: 0.4883 - val_loss: 1.1891 - val_accuracy: 0.5234\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.2021 - accuracy: 0.4869 - val_loss: 1.1931 - val_accuracy: 0.5060\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - 39s 232ms/step - loss: 1.2043 - accuracy: 0.4832 - val_loss: 1.1635 - val_accuracy: 0.5312\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.1957 - accuracy: 0.4903 - val_loss: 1.1268 - val_accuracy: 0.5032\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - 39s 234ms/step - loss: 1.1995 - accuracy: 0.4842 - val_loss: 1.1803 - val_accuracy: 0.5252\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - 38s 232ms/step - loss: 1.1842 - accuracy: 0.4935 - val_loss: 0.9376 - val_accuracy: 0.5179\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.2001 - accuracy: 0.4850 - val_loss: 1.3682 - val_accuracy: 0.5217\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - 38s 230ms/step - loss: 1.1859 - accuracy: 0.4911 - val_loss: 1.0817 - val_accuracy: 0.5344\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1914 - accuracy: 0.4935 - val_loss: 1.1055 - val_accuracy: 0.5113\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - 38s 232ms/step - loss: 1.1971 - accuracy: 0.4847 - val_loss: 1.1048 - val_accuracy: 0.5280\n",
      "Epoch 18/30\n",
      "166/166 [==============================] - 38s 232ms/step - loss: 1.1878 - accuracy: 0.4955 - val_loss: 1.1346 - val_accuracy: 0.5399\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - 38s 230ms/step - loss: 1.1872 - accuracy: 0.4932 - val_loss: 0.5023 - val_accuracy: 0.4977\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - 39s 234ms/step - loss: 1.1810 - accuracy: 0.4896 - val_loss: 1.0367 - val_accuracy: 0.5061\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1912 - accuracy: 0.4938 - val_loss: 1.1660 - val_accuracy: 0.5312\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - 38s 231ms/step - loss: 1.1782 - accuracy: 0.4968 - val_loss: 0.9747 - val_accuracy: 0.5316\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - 39s 236ms/step - loss: 1.1829 - accuracy: 0.4979 - val_loss: 1.1449 - val_accuracy: 0.5226\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1873 - accuracy: 0.4933 - val_loss: 1.0975 - val_accuracy: 0.5362\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - 39s 234ms/step - loss: 1.1753 - accuracy: 0.4933 - val_loss: 1.1067 - val_accuracy: 0.5165\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - 39s 235ms/step - loss: 1.1834 - accuracy: 0.4966 - val_loss: 1.3269 - val_accuracy: 0.5454\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - 39s 232ms/step - loss: 1.1813 - accuracy: 0.4908 - val_loss: 1.1343 - val_accuracy: 0.5061\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - 39s 234ms/step - loss: 1.1792 - accuracy: 0.4987 - val_loss: 1.1188 - val_accuracy: 0.5380\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - 39s 235ms/step - loss: 1.1870 - accuracy: 0.4937 - val_loss: 1.2187 - val_accuracy: 0.5182\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - 38s 232ms/step - loss: 1.1707 - accuracy: 0.5008 - val_loss: 1.2477 - val_accuracy: 0.5390\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"age_prediction_50epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"age_prediction_50.h5\",\n",
    "                             monitor=\"val_accuracy\",\n",
    "                             mode=\"max\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "166/166 [==============================] - 37s 221ms/step - loss: 1.1642 - accuracy: 0.5039 - val_loss: 1.1210 - val_accuracy: 0.5139\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.51389, saving model to age_prediction_50.h5\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 37s 223ms/step - loss: 1.1831 - accuracy: 0.4957 - val_loss: 1.1627 - val_accuracy: 0.5391\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.51389 to 0.53906, saving model to age_prediction_50.h5\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 38s 229ms/step - loss: 1.1738 - accuracy: 0.5020 - val_loss: 1.0617 - val_accuracy: 0.5041\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.53906\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 37s 225ms/step - loss: 1.1790 - accuracy: 0.4932 - val_loss: 1.0802 - val_accuracy: 0.5451\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.53906 to 0.54514, saving model to age_prediction_50.h5\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 38s 226ms/step - loss: 1.1667 - accuracy: 0.5026 - val_loss: 0.9625 - val_accuracy: 0.5472\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.54514 to 0.54720, saving model to age_prediction_50.h5\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 38s 226ms/step - loss: 1.1773 - accuracy: 0.4966 - val_loss: 0.9812 - val_accuracy: 0.5217\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.54720\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1734 - accuracy: 0.5018 - val_loss: 1.0036 - val_accuracy: 0.5197\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.54720\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1705 - accuracy: 0.5008 - val_loss: 1.1565 - val_accuracy: 0.5208\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.54720\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 39s 232ms/step - loss: 1.1641 - accuracy: 0.5032 - val_loss: 1.3798 - val_accuracy: 0.5454\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.54720\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 39s 232ms/step - loss: 1.1769 - accuracy: 0.5011 - val_loss: 1.2238 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.54720\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 40s 239ms/step - loss: 1.1727 - accuracy: 0.4990 - val_loss: 1.1779 - val_accuracy: 0.5417\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.54720\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 40s 238ms/step - loss: 1.1599 - accuracy: 0.5095 - val_loss: 1.1689 - val_accuracy: 0.5095\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.54720\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 39s 238ms/step - loss: 1.1694 - accuracy: 0.5029 - val_loss: 1.0870 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.54720 to 0.54995, saving model to age_prediction_50.h5\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 39s 233ms/step - loss: 1.1732 - accuracy: 0.5017 - val_loss: 1.0924 - val_accuracy: 0.5391\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.54995\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 39s 235ms/step - loss: 1.1569 - accuracy: 0.5037 - val_loss: 1.0418 - val_accuracy: 0.5298\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.54995\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 39s 237ms/step - loss: 1.1644 - accuracy: 0.5032 - val_loss: 1.0973 - val_accuracy: 0.5234\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.54995\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 38s 226ms/step - loss: 1.1739 - accuracy: 0.4960 - val_loss: 1.1420 - val_accuracy: 0.5380\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.54995\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 38s 230ms/step - loss: 1.1702 - accuracy: 0.4957 - val_loss: 1.1365 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.54995\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 38s 226ms/step - loss: 1.1582 - accuracy: 0.5066 - val_loss: 0.8185 - val_accuracy: 0.5261\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.54995\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 39s 236ms/step - loss: 1.1637 - accuracy: 0.5018 - val_loss: 1.1885 - val_accuracy: 0.5226\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.54995\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
